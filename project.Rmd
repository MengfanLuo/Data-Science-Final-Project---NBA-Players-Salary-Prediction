---
title: "P8106 - Midterm Project - NBA Players Salary Prediction"
author: "Mengfan Luo (ml4701)"
date: "03/26/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(caret)
library(patchwork)
library(mgcv)
library(earth)
library(corrplot)
library(vip)

```


## Introduction

NBA players are considered to be among the highest-paid athletes in the world. Despite even the minimum wage in the NBA is way higher than that of any professional league in North America, there's a large gap between NBA stars and ordinary players[1]. In this project, we are interested in finding the factors that influence salary of NBA players. We will also develop a model to predict the salary.

We will conduct data analysis and model construction based on two datasets on NBA players' contracted salary [2] and performance statistics per game [3] in 2021-2022. The following steps are included in our data preperation:

 - Two original datasets are inner joined by players and teams
 - Keep only one record with most number of games played for each of players, given a player may transfer to other teams during the session and have multiple records.
 - Remove 5 variables with missing values caused by division of other existing variables. 
 - Remove variable representing the player name. 

The final cleaned dataset has 442 records and 24 variables. 

```{r joining datasets}
df_salary = read_csv("NBA_season2122_player_salary.csv") %>%
  janitor::clean_names() %>%
  select(Player=x2,Team=x3,Salary=salary_4) %>%
  na.omit()

df_salary = df_salary[-1,]

df_stats = read_csv("NBA_season2122_player_stats.csv") %>%
  rename(Team=Tm) %>%
  select(-Rk)

df_players = inner_join(x=df_salary,y=df_stats,by=c("Player","Team")) %>% 
  janitor::clean_names() %>% 
  distinct()

df_players = df_players %>% 
  arrange(player,desc(g)) %>% 
  distinct(player,.keep_all = TRUE)

# Removed variables with missing data and resulted from division of other variables
df_players = df_players %>% 
  select(-x3p_percent, -ft_percent, -fg_percent,-x2p_percent,-e_fg_percent,-player)

#The final generated dataset for use: df_player.
```


```{r data cleaning}
# Convert salary from characters to numbers.
# Convert categorical variables to factors

df_players = df_players %>% 
  separate(salary,into = c("symbol", "salary"),1) %>% 
  select(-symbol)%>% 
  mutate(salary = as.numeric(salary)/1000000,
         team = factor(team),
         pos = factor(pos))%>% 
  relocate(salary, .after = last_col())

```

## Exploratory analysis and visualization

There are 24 variables in our cleaned dataset, with 2 categorical variables, 21 numerical variables and 1 numeric response variable `salary`. A summary of predictors are provided in appendix A.

Looking into the variables, we find some variables can be derived from other variables by simple addition. For example, `fg = x3p+x2p`, number of field goals = summation of number of 3-point goals and 2-point goals. Therefore, 3 grouped variables `fg`,`fga`, `trb` are removed. Since `mp` stands for minutes played per game, we will divided variables stands for counts by `mp` to get a rate. These variables includes `x3p`,`x3pa`,`x2p`, `x2pa`, `ft`, `fta`, `orb`, `drb`, `ast`, `stl`, `blk`, `tov`, `pf` and `pts`.

```{r}
df_players = df_players %>% 
  select(-fg,-fga,-trb) %>% 
  mutate(x3p = x3p/mp,
         x3pa = x3pa/mp,
         x2p = x2p/mp,
         x2pa = x2pa/mp,
         ft = ft/mp,
         fta = fta/mp,
         orb = orb/mp,
         drb = drb/mp,
         ast = ast/mp,
         stl = stl/mp,
         blk = blk/mp,
         tov = tov/mp,
         pf = pf/mp,
         pts = pts/mp) 
```

\newpage

### Univariate analysis

The following plots show distribution of each univariable. For categorical variables `team` and `pos`, they are dsitributed quite evenly. There are 30 unique values in `team` and 5 in `pos`. We may consider remove `team` for it may result in too many dummy variables in the model.

```{r fig.height = 3}
par(mfrow=c(1,2))
plot(df_players$team, main = "team")
plot(df_players$pos, main = "pos")
```

For numeric variables, some of them (`gs`, `ft`, `orb`,`blk`), including response `salary` are skewed, with some players have extremely high salary. Visualization for all variables are enclosed in Appendix B


```{r fig.height = 4}

df_appendix_b = df_players

par(mfrow=c(2,3))
hist(df_players$gs)
hist(df_players$ft)
hist(df_players$orb)
hist(df_players$blk)
hist(df_players$salary)
```

\newpage

### Correlation Analysis

From correlation plot for remaining numeric variable (left), we find rate of goals (`x3p`,`x2p`,and `fp`) and rate of goal attempts (`x3pa`,`x2pa`,and `fpa`) are highly correlated. Therefore, all goal attempts will be further excluded. Besides, `g`, `orb`,`blk`, `stl`, and `drb` are lowly correlated with response `salary`, so we may remove them to reduce variance. The resulting correlation plot is shown in the right. Still some high correlation remains (`pts` and `x2p`, `pts` and `ft`, `mp` and `gs`).

```{r fig.height = 3}
par(mfrow=c(1,2))

df_corr_1 = df_players %>% 
  select(-team,-pos)

corrplot(cor(df_corr_1),type = "lower")

df_corr_2 = df_players %>% 
  select(-team,-pos, -x3pa,-x2pa,-fta,-g,-stl,-blk, -drb, -orb)


corrplot(cor(df_corr_2),type = "lower")
```


```{r}
# Remove high correlated variables and categorical variable team.
df_players = df_players %>% 
  select(-team, -x3pa,-x2pa,-fta,-g,-stl,-blk, -drb, -orb)
```

\newpage

### Analyzing trends in data

From numeric variables, we found that `stl`,`x3p`, `age`,`gs` seem to have some non-linear trends.

```{r, fig.height=4}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

featurePlot(df_players[2:11], df_players$salary, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(5,2))
```

From categorical variable `pos`, extremely high values in salary show in all positions and some teams.

```{r fig.height=4}
df_players %>% 
  mutate(
    pos = fct_reorder(pos,salary)
  ) %>% 
  ggplot(aes(x = pos, y = salary, group = pos, fill = pos))+
  geom_boxplot()

```

\newpage

## Models

### Data Partition and Transformation

From above exploratory analysis, we removed and engineered some variables. There are 11 variables, `pos`,`age`,`gs` ,`mp` ,`x3p`,`x2p`,`ft` ,`ast`, `tov`,`pf` and `pts` that will be used for further model building.

We partition the dataset into 334 training data (75%) and 108 test data (25%). We will use 10-fold CV to compare each model using training data, and then select a best model to predict on test data.

```{r}
# Data partition
set.seed(4701)

indexTrain <- createDataPartition(y = df_players$salary, p = 0.75, list = FALSE, times = 1)
trainData <- df_players[indexTrain, ]
testData <- df_players[-indexTrain, ]

x_train <- model.matrix(salary~.,trainData)[,-1]
y_train <- trainData$salary

x_test <- model.matrix(salary~.,testData)[,-1]
y_test <- testData$salary

ctrl1 <- trainControl(method = "cv", number = 10)
```

Since there are some correlation with variables, methods such as PLS will be considered to reduce variance. Also, since there were some non-leanrity in some variables, methods such as GAM will also be considered.

Because these above methods are all parametric methods and based on assumption of normality, we considering doing transformation on the numeric predictors. We will use Yeo-Johnson transformation because there are many 0 value in our predictors.

```{r}
# preprocessing
#Yeo-Johnson transformation on X

x_numeric_train = trainData[2:11]
x_numeric_test = testData[2:11]

pp = preProcess(x_numeric_train,method = c("YeoJohnson"),"zv")

x_train_pp = predict(pp, x_numeric_train)
x_test_pp = predict(pp,x_numeric_test)

## Combine transformed numeric variables and dummy variables
x_train = cbind(x_train[1:334,1:4],x_train_pp[1:334,1:10])
x_test = cbind(x_test[1:108,1:4],x_test_pp[1:108,1:10])

```


### Lasso

Lasso is a linear model with a penalty term. The parameter lambda is tried from exp(-9) to exp(-1), and picked based on lowest CV RMSE. Here lambda = 0.001093708. All variables `pos` ,`age`,    `gs`, `mp` ,  `x3p`  ,`x2p`  ,  `ft` ,  `ast` , `tov`, `pf` and   `pts` are included. Lasso is good interms of iterprebility. we can see from the coeficients that `pf` and `pts` are inversely correlated with salary, while the others are positively correlated. However, `pts` is the rate of getting points each game, so it doesn't make sense that the more points one player can get in unit time, the less salary he has. This problem may caused by correlation between `pts` and several other variables. Lasso should be able to perform function of variable selection, but here all variables are selected into the final model. Another drawback of lasso is that it is not able to deal with possible non-linear trend in the predictors.

\newpage

```{r fig.height = 4}
set.seed(4701)
fit_lasso <- train(x_train, y_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-1, -9, length=100))),
                   preProcess = c("center","scale"),
                   trControl = ctrl1)

plot(fit_lasso, xTrans = log)
```


```{r fig.height=4}
plot(fit_lasso$finalModel)
```

\newpage

### PLS

For PLS, the parameter is number of components ncomp. n = 13 is selected for lowest CV RMSE. All variables are included to form the 13 components. PLS can reduce the dimension of predictors, but possible drawback of PLS are that it cannot handle interactions between original variables, as well as cannot handle non-linerity.

```{r fig.height = 4}
set.seed(4701)
pls.fit <- train(x_train, y_train,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:14),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

ggplot(pls.fit, highlight = TRUE)
```

\newpage

### GAM

For GAM, there's no parameters. From the model, variables except for `gs` are included. The degrees of freedom are shown as followed. The GAM model can capture the non-linear trend in the model, but it may have a high variance.

```{r}
set.seed(4701)
gam.fit <- train(x_train, y_train,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 trControl = ctrl1)

tibble(
  var = c("pos","age" ,   "mp"    , "x3p"  ,  "x2p"  ,  "ft"   ,  "ast" ,   "tov"  ,"pf"  ,   "pts"),
  df = c(NA, 6.508,  3.099, 2.283, 0.178, 7.345, 7.330 ,1.754, 3.427, 2.199)
)%>% 
  kableExtra::kable()


```


### Model Comparison
 
The CV RMSE are showned as followed. We can see GAM model has lowest RMSE. Using GAM model to do prediction, the test RMSE is 5.517879, quite similar to CV RMSE results.
 
```{r}
bwplot(resamples(list(
               lasso = fit_lasso,
               pls = pls.fit,
               gam = gam.fit)),
       metric = "RMSE")

y_pred <- predict(pls.fit, newdata = x_test)
test_rmse = sqrt(mean((y_pred - y_test)^2))

```


 
## Conclusions

From the above analysis and model, we can predict salary of NBA players with GAM model. Predictors includes `pos` ,   `age`,    `mp` ,    `x3p`  ,  `x2p`  ,  `ft`  ,   `ast` ,   `tov`, `pf` ,    `pts`, and the corresponding degree of freedoms and interpretations are as followed. We can see that GAM model have a low bias, possibly thanks to its ability of capturing the non-linear trend in the model. Also, from the bloxplot above, GAM model also has lowest variance in the three models, which is quite ideal. 
 
 
```{r}
tibble(
  var = c("pos","age" ,   "mp"    , "x3p"  ,  "x2p"  ,  "ft"   ,  "ast" ,   "tov"  ,"pf"  ,   "pts"),
  df = c(NA, 6.508,  3.099, 2.283, 0.178, 7.345, 7.330 ,1.754, 3.427, 2.199),
  interpret = c(
    "position of player",
    "age of player",
    "miniutes played per game",
    "rate making of 3-point goal",
    "rate of making 2-point goal",
    "rate of making free throw",
    "rate of assistance",
    "rate of turn over",
    "rate of personal fouls",
    "rate of getting points in each game"
  )
  
) %>% 
  kableExtra::kable()

```


## References

[1]https://www.hoopsrumors.com/2021/08/nba-minimum-salaries-for-2021-22.html

[2]https://www.basketball-reference.com/contracts/players.html

[3]https://www.basketball-reference.com/leagues/NBA_2022_per_game.html

\newpage

## Appendices

### Appendix A - Variables in Original Dataset

 - salary: salary of the player in millions (Response)
 - pos -- Position of the player (5 categories)
 - age -- Player's age on February 1 of the season
 - team -- Team that the player belong to. (30 categories)
 - g -- Number of Games Participated
 - gs -- Number of Games Started
 - mp -- Minutes Played Per Game
 - fg -- Field Goals Per Game (Count)
 - fga -- Field Goal Attempts Per Game (Count)
 - x3p -- 3-Point Field Goals Per Game (Count)
 - x3pa -- 3-Point Field Goal Attempts Per Game (Count)
 - x2p -- 2-Point Field Goals Per Game (Count)
 - x2pa -- 2-Point Field Goal Attempts Per Game (Count)
 - ft -- Free Throws Per Game  (Count)
 - fta -- Free Throw Attempts Per Game (Count)
 - orb -- Offensive Rebounds Per Game (Count)
 - drb -- Defensive Rebounds Per Game (Count)
 - trb -- Total Rebounds Per Game (Count)
 - ast -- Assists Per Game (Count)
 - stl -- Steals Per Game  (Count)
 - blk -- Blocks Per Game (Count)
 - tov -- Turnovers Per Game (Count)
 - pf -- Personal Fouls Per Game (Count)
 - pts -- Points Per Game

\newpage


### Appendix B - Variable Distribution

```{r fig.height = 9}
par(mfrow=c(5,4))
hist(df_appendix_b$age)
hist(df_appendix_b$g)
hist(df_appendix_b$gs)
hist(df_appendix_b$mp)
hist(df_appendix_b$x3p)
hist(df_appendix_b$x3pa)
hist(df_appendix_b$x2p)
hist(df_appendix_b$x2pa)
hist(df_appendix_b$ft)
hist(df_appendix_b$fta)
hist(df_appendix_b$orb)
hist(df_appendix_b$drb)
hist(df_appendix_b$ast)
hist(df_appendix_b$stl)
hist(df_appendix_b$blk)
hist(df_appendix_b$tov)
hist(df_appendix_b$pf)
hist(df_appendix_b$pts)
hist(df_appendix_b$salary)
```



