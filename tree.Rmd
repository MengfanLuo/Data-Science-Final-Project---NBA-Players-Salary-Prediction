---
title: "Random Forest and Boosting"
author: "DS2"
date: "5/6/2022"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,dpi=300)
library(tidyverse)
library(caret)
library(patchwork)
library(mgcv)
library(earth)
library(corrplot)
library(vip)
library(ggpubr)
library(ranger)
library(gbm)
library(factoextra)
library(lime)
```


## Part 0 - Data Preprocessing

```{r joining datasets}
df_salary = read_csv("NBA_season2122_player_salary.csv") %>%
  janitor::clean_names() %>%
  select(Player=x2,Team=x3,Salary=salary_4) %>%
  na.omit()

df_salary = df_salary[-1,]

df_stats = read_csv("NBA_season2122_player_stats.csv") %>%
  rename(Team=Tm) %>%
  select(-Rk)

df_players = inner_join(x=df_salary,y=df_stats,by=c("Player","Team")) %>% 
  janitor::clean_names() %>% 
  distinct()

df_players = df_players %>% 
  arrange(player,desc(g)) %>% 
  distinct(player,.keep_all = TRUE)

# Removed variables with missing data and resulted from division of other variables
df_players = df_players %>% 
  select(-x3p_percent, -ft_percent, -fg_percent,-x2p_percent,-e_fg_percent)

# The final generated dataset for use: df_player.
```


```{r data cleaning}
# Convert salary from characters to numbers.
# Convert categorical variables to factors

df_players = df_players %>% 
  separate(salary,into = c("symbol", "salary"),1) %>% 
  select(-symbol)%>% 
  mutate(salary = as.numeric(salary)/1000000,
         team = factor(team),
         pos = factor(pos)) %>% 
  relocate(salary, .after = last_col())

colnames(df_players) = c("player", "team", "position", "age", "game","game_starting" ,"minute","field_goal", "fg_attempt", "x3p", "x3p_attempt" ,"x2p", "x2p_attempt",   "free_throw",   "ft_attempt", "offensive_rb", "defenssive_rb",  "total_rb" ,   "assistance" ,   "steal" , "block",    "turnover",  "personal_foul", "point", "salary")

```

## Part 1 - Exploratory Analysis 

Since `minute` stands for minutes played per game, we will divided variables stands for counts by `minute` to get a rate. These variables includes `field_goal`, `fg_attempt`    `x3p`, `x3p_attempt`, `x2p`, `x2p_attempt`,   `free_throw`,  `ft_attempt`, `offensive_rb`  `defenssive_rb`, `total_rb`, `assistance`,`steal`, `block`, `turnover`, `personal_foul` and `point`.

```{r}
df_players = df_players %>% 
  mutate(field_goal = field_goal/minute,
         fg_attempt = fg_attempt/minute,
         x3p = x3p/minute,
         x3p_attempt = x3p_attempt/minute,
         x2p = x2p/minute,
         x2p_attempt = x2p_attempt/minute,
         free_throw = free_throw/minute,
         ft_attempt = ft_attempt/minute,
         offensive_rb = offensive_rb/minute,
         defenssive_rb = defenssive_rb/minute,
         total_rb = total_rb/minute,
         assistance = assistance/minute,
         steal = steal/minute,
         block = block/minute,
         turnover = turnover/minute,
         personal_foul = personal_foul/minute,
         point = point/minute) 
```



### Univariate Analysis

Distributions of the two categorical variables, `team` and `position`.

```{r}
par(mfrow=c(1,2))
plot_team = ggplot(df_players) + geom_bar(aes(team)) + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))+ theme_bw()
plot_team
plot_position = ggplot(df_players) + geom_bar(aes(position)) + theme_bw()
plot_position
```

Distributions of other numeric variables. 

```{r dpi=300}
plot_data_column = function (data, column) {
    ggplot(data, aes_string(x = column)) +
        geom_histogram(bins=15) +
        xlab(column) + theme_bw(base_size = 10)
}

histograms <- lapply(colnames(df_players)[4:24], 
                       plot_data_column, data = df_players)

figure_a = ggarrange(plotlist = histograms[1:9], 
          ncol = 3, nrow = 3)

annotate_figure(figure_a, 
                top = text_grob("Histograms of Predictive Variables (Group A)", 
                                face = "bold", size = 15))


figure_b = ggarrange(plotlist = histograms[10:18], 
          ncol = 3, nrow = 3)
annotate_figure(figure_b, 
                top = text_grob("Histograms of Predictive Variables (Group B)", 
                                face = "bold", size = 15))

figure_c = ggarrange(plotlist = histograms[19:21],
          ncol = 3, nrow = 3)
annotate_figure(figure_c, 
                top = text_grob("Histograms of Predictive Variables (Group C)", 
                                face = "bold", size = 15))
```


### Correlation Analysis

```{r}

df_corr_1 = df_players %>% 
  select(-player, -team,-position)

corrplot(cor(df_corr_1),type = "lower")

```


### Analyzing trends in data

From numeric variables, we found that `stl`,`x3p`, `age`,`gs` seem to have some non-linear trends.

```{r, fig.height=4}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(0, 0, 0, 1) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(1, .1, .1, 1) 
theme1$plot.line$lwd <- 2 
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)

df_features = df_players[4:24]
featurePlot(x = df_features, 
            y = df_players$salary,
            plot = "scatter",
            # span = .5,
            labels = c("Predictors","Player Salary (Millions)"), 
            type = c("p", "smooth"), 
            layout = c(7, 3))
```

From categorical variable `position`, extremely high values in salary show in all positions and some teams.

```{r fig.height=4}
df_players %>% 
  mutate(
    pos = fct_reorder(position,salary)
  ) %>% 
  ggplot(aes(x = position, y = salary, group = pos, fill = pos))+
  geom_boxplot() + theme_bw()

```


```{r}
# Data partition
set.seed(8106)

indexTrain <- createDataPartition(y = df_players$salary, p = 0.75, list = FALSE, times = 1)
ctrl1 <- trainControl(method = "cv", number = 10, repeats = 5)
```


## Tree-based models

### With variable `team`

#### 1-1. Random forest

```{r}

rf.grid <- expand.grid(
  mtry = 30:54,
  splitrule = "variance",
  min.node.size = 1:6)

set.seed(8106)
rf.fit <- train(salary ~ . , 
                df_players[indexTrain,][2:25],
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)

rf.fit$bestTune

ggplot(rf.fit, highlight = TRUE)
```


#### 1-2. gbm

```{r}
gbm.grid <- expand.grid(n.trees = c(2000,3000,4000,5000),
                        interaction.depth = 1:5,
                        shrinkage = c(0.0008,0.001,0.002),
                        n.minobsinnode = 1)

set.seed(8106)
gbm.fit <- train(salary ~ . , 
                 df_players[indexTrain,][2:25], 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl1,
                 verbose = FALSE)
gbm.fit$bestTune

ggplot(gbm.fit, highlight = TRUE)

gbm.fit$finalModel

```


### Without variable `team`

#### 2-1. Random forest

```{r}

rf.grid2 <- expand.grid(
  mtry = 5:25,
  splitrule = "variance",
  min.node.size = 1:6)

set.seed(8106)
rf.fit2 <- train(salary ~ . , 
                df_players[indexTrain,][3:25],
                method = "ranger",
                tuneGrid = rf.grid2,
                trControl = ctrl1)

rf.fit2$bestTune

ggplot(rf.fit2, highlight = TRUE)
```


#### 2-2. gbm

```{r}
gbm.grid2 <- expand.grid(n.trees = c(2000,3000,4000,5000),
                        interaction.depth = 1:5,
                        shrinkage = c(0.0008,0.001,0.002),
                        n.minobsinnode = 1)

set.seed(8106)
gbm.fit2 <- train(salary ~ . , 
                 df_players[indexTrain,][3:25], 
                 method = "gbm",
                 tuneGrid = gbm.grid2,
                 trControl = ctrl1,
                 verbose = FALSE)
gbm.fit2$bestTune

ggplot(gbm.fit2, highlight = TRUE)

gbm.fit2$finalModel

```


### With variable `team` clustered into 3 clusters

Categorical variable `team`have 30 classes, which will resulted in too much dummy variables in our models. Therefore, we consider clustering `team` into fewer class according to similar trends in the median and standard deviation of player's salary in each team.

```{r}
df_team = df_players[indexTrain,] %>% 
  group_by(team) %>% 
  summarize(median = median(salary),
            sd = sd(salary)) %>% 
  mutate(team = as.character(team))

df_team1 = data.frame(median = df_team$median, sd = df_team$sd)
rownames(df_team1) = df_team$team
df_team1  = scale(df_team1)

```


We use k-mean clustering to cluster variable `team` in the training data with class number k = 3. Variable `team`  are clustered into the following 3 clusters:

* Cluster 1: CHO, DAL, DEN, DET, HOU, IND, NYK, OKC, ORL, POR, SAS, TOR
* Cluster 2: ATL, BOS, CHI, CLE, LAC, MEM, MIN, SAC, UTA
* Cluster 3: BRK, GSW, LAL, MIA, MIL, NOP, PHI, PHO, WAS

```{r, fig.height=3.5}

set.seed(8106)
fviz_nbclust(df_team1,
             FUNcluster = kmeans,
             method = "silhouette")

km <- kmeans(df_team1, centers = 3, nstart = 30)

km_vis <- fviz_cluster(list(data = df_team1, cluster = km$cluster), 
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "K-means") 

km_vis

team_dict = data.frame(
  team = df_team$team,
  team_cluster = factor(unname(km$cluster))
)

```


We add class labels for the newly generated clusters of `team` as `team_cluster`, with values 1, 2, and 3 representing each clusters.

```{r}
df_players2 = inner_join(x = df_players,y = team_dict,by = "team") %>% 
  relocate(team_cluster, .before = team) %>% 
  select(-team)
  
```


#### 3-1. Random forest

```{r}

rf.grid3 <- expand.grid(
  mtry = 5:27,
  splitrule = "variance",
  min.node.size = 1:6)

set.seed(8106)
rf.fit3 <- train(salary ~ . , 
                df_players2[indexTrain,][2:25],
                method = "ranger",
                tuneGrid = rf.grid3,
                trControl = ctrl1)

rf.fit3$bestTune

ggplot(rf.fit3, highlight = TRUE)
```


#### 3-2. gbm

```{r}
gbm.grid3 <- expand.grid(n.trees = c(1000,2000,3000,4000,5000),
                        interaction.depth = 2:6,
                        shrinkage = c(0.001,0.002,0.003),
                        n.minobsinnode = 1)

set.seed(8106)
gbm.fit3 <- train(salary ~ . , 
                 df_players2[indexTrain,][2:25], 
                 method = "gbm",
                 tuneGrid = gbm.grid3,
                 trControl = ctrl1,
                 verbose = FALSE)
gbm.fit3$bestTune

ggplot(gbm.fit3, highlight = TRUE)

gbm.fit3$finalModel

```


Comparison of Tree-based models

```{r}
resamp <- resamples(list(
  rf = rf.fit, 
  gbm = gbm.fit,
  rf_no_team = rf.fit2, 
  gbm_no_team = gbm.fit2,
  rf_cluster_team = rf.fit3, 
  gbm_cluster_team = gbm.fit3))

rmse_resamp = unname(summary(resamp)$statistics$RMSE[,4])

bwplot(resamples(list(
               rf = rf.fit,
               gbm = gbm.fit,
               rf_no_team = rf.fit2, 
               gbm_no_team = gbm.fit2,
               rf_cluster_team = rf.fit3, 
               gbm_cluster_team = gbm.fit3)),
       metric = "RMSE")

data.frame(
  model = summary(resamp)$models,
  cv_rmse = rmse_resamp)

```


